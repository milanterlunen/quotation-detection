{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47c3c3a",
   "metadata": {},
   "source": [
    "# Analyze text data\n",
    "\n",
    "This notebook serves to analyze data on two kinds of text from the results:\n",
    "- quoted passages as they appear in the source text (quoted passages in A) and as they appear in the target corpus (quoted passages in B)\n",
    "- context chunks to the left and right of each quoted passage, as it appears in the target corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503f1ef",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f8a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from text_matcher.matcher import Text, Matcher\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75fc130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION: copy path to results JSONL file here (filename should end \"_results_[hyperparameters].jsonl\")\n",
    "\n",
    "startData = \"/Users/milan/Library/CloudStorage/GoogleDrive-mtt2126@columbia.edu/My Drive/iAnnotate/MIT/Quotable Content/Data/Price/2000_AnthologyRise/Results/Price_2000_AnthologyRise_results_t2-c3-n2-m3-nostops.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9339de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author surname: Price\n",
      "Publication year: 2000\n",
      "Text title: AnthologyRise\n",
      "Hyperparameters suffix: _t2-c3-n2-m3-nostops\n",
      "Data directory:/Users/milan/Library/CloudStorage/GoogleDrive-mtt2126@columbia.edu/My Drive/iAnnotate/MIT/Quotable Content/Data\n"
     ]
    }
   ],
   "source": [
    "# Infer naming variables from path\n",
    "\n",
    "textTitle = startData.rsplit(\"_\", 4)[-3]\n",
    "publicationYear = startData.rsplit(\"_\", 4)[-4]\n",
    "authorSurname = startData.rsplit(\"_\", 4)[-5]\n",
    "authorSurname = authorSurname.rsplit(\"/\", 1)[-1]\n",
    "hyperparSuffix = startData.rsplit(\"_\", 4)[-1]\n",
    "hyperparSuffix = f\"_{hyperparSuffix[:-6]}\"\n",
    "dataDir = startData.rsplit(\"/\", 4)[0]\n",
    "\n",
    "print(f\"Author surname: {authorSurname}\\nPublication year: {publicationYear}\\nText title: {textTitle}\\nHyperparameters suffix: {hyperparSuffix}\\nData directory:{dataDir}\")\n",
    "\n",
    "projectName = f\"{authorSurname}_{publicationYear}_{textTitle}\"\n",
    "sourceDir = f\"{dataDir}/{authorSurname}/{publicationYear}_{textTitle}/SourceText\"\n",
    "corpusDir = f\"{dataDir}/{authorSurname}/{publicationYear}_{textTitle}/TargetCorpus\"\n",
    "resultsDir = f\"{dataDir}/{authorSurname}/{publicationYear}_{textTitle}/Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88d826f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1687 results from text-matcher\n"
     ]
    }
   ],
   "source": [
    "# Load in the JSONL file with the results of text matcher\n",
    "\n",
    "df = pd.read_json(f\"{startData}\", lines=True)\n",
    "print(f\"Loaded {len(df)} results from text-matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa09739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns present.\n",
      "All columns are filled. You can proceed!\n"
     ]
    }
   ],
   "source": [
    "# Check that the results file includes data on quoted passages and context chunks\n",
    "# (Some older datasets didn't include this.)\n",
    "\n",
    "# Check that the df includes the relevant columns\n",
    "\n",
    "if {'quotedPassageinA', 'quotedPassageinB', 'contextChunkLeft', 'contextChunkRight'}.issubset(df.columns):\n",
    "    print(\"All columns present.\")\n",
    "\n",
    "    # Check that all columns are completely filled\n",
    "    \n",
    "    if all(df[['quotedPassageinA', 'quotedPassageinB', 'contextChunkLeft', 'contextChunkRight']].notnull().all(axis=0)) == True:\n",
    "        print(\"All columns are filled. You can proceed!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Some columns are missing data. Return to phase 02 to troubleshoot.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: columns missing. Run the results through Notebook 'add-quoted-passages-context-chunks-to-results'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c856f",
   "metadata": {},
   "source": [
    "# Output entire left/right contexts and quoted passages as text files (eg for word cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37525dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.to_string of 0       [ zur Zephyr. Motorrader, die Geschichte macht...\n",
      "1                                                      []\n",
      "2       [colonies. Request complete 12-page Heavy Oxyg...\n",
      "3                                                      []\n",
      "4                                                      []\n",
      "                              ...                        \n",
      "1682    [iversity Press, 41 William Street, Prince- Un...\n",
      "1683    [7. Existentialism and humanism. London: build...\n",
      "1684    [er company's catalogue, we can supply it to y...\n",
      "1685                                                   []\n",
      "1686    [ its functions. The book can be employed eith...\n",
      "Name: contextChunkLeft, Length: 1687, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "# Resume here: trying to output relevant df columns as string separated by newline\n",
    "\n",
    "testString = df[\"contextChunkLeft\"].to_string\n",
    "\n",
    "print(testString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d748f10b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresultsDir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprojectName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-allQuotationContexts.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontextChunkLeft\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     output_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#for chunk in allChunksRight:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#    output_file.write(chunk)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#    output_file.write('\\n')\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not list"
     ]
    }
   ],
   "source": [
    "# Output all left/right contexts as single text file\n",
    "\n",
    "output_file = open(f'{resultsDir}/{projectName}-allQuotationContexts.txt', mode='w', encoding='utf-8')\n",
    "\n",
    "for chunk in allChunksLeft:\n",
    "    output_file.write(chunk)\n",
    "    output_file.write('\\n')\n",
    "\n",
    "for chunk in allChunksRight:\n",
    "    output_file.write(chunk)\n",
    "    output_file.write('\\n')\n",
    "    \n",
    "output_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all quoted passages as single text file\n",
    "\n",
    "output_file = open(f'{resultsDir}/{projectName}-allQuotedPassagesinB.txt', mode='w', encoding='utf-8')\n",
    "for passage in allQuotedPassagesinB:\n",
    "    output_file.write(passage)\n",
    "    output_file.write('\\n')\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800ea87",
   "metadata": {},
   "source": [
    "# OPTIONAL: reimport JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0f6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f\"{resultsDir}/{projectName}_results{hyperparSuffix}_contextChunks.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62697ac4",
   "metadata": {},
   "source": [
    "\n",
    "# Calculate percentage change for keywords over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3743bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge left Chunk and right Chunk for each subset\n",
    "df[\"fullContextChunks\"] = df[\"contextChunksLeft\"] + \" \" + df[\"contextChunksRight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08ce7c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     defin the channel within which resourc flow. ...\n",
       "1     the university, manchest mi3 9pl. o'higgins, ...\n",
       "2    sequilibrium. this is particular the case in d...\n",
       "3    properti and that the wealthiest among them in...\n",
       "4    eyes, in my own hall. i'll find out how that d...\n",
       "Name: stemmedChunks, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem all text\n",
    "\n",
    "import nltk\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "df['stemmedChunks'] = df.fullContextChunks.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "df.stemmedChunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b55a8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify year at which the second period should start\n",
    "\n",
    "transitionPoint = 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63e94fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in first period up to (but excluding) 2010: 4893\n",
      "Items in second period starting from (and including) 2010: 1455\n"
     ]
    }
   ],
   "source": [
    "# Create lists to contain text chunks\n",
    "\n",
    "firstPeriodChunks = []\n",
    "secondPeriodChunks = []\n",
    "\n",
    "# Append stemmed chunks to respective lists\n",
    "\n",
    "for item in range(len(df)):\n",
    "    currentChunk = df['stemmedChunks'].iloc[item]\n",
    "    if df['Year'].iloc[item] < transitionPoint:\n",
    "        firstPeriodChunks.append(currentChunk)\n",
    "    if df['Year'].iloc[item] >= transitionPoint:\n",
    "        secondPeriodChunks.append(currentChunk)\n",
    "        \n",
    "print(f\"Items in first period up to (but excluding) {transitionPoint}: {len(firstPeriodChunks)}\")\n",
    "print(f\"Items in second period starting from (and including) {transitionPoint}: {len(secondPeriodChunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2adf6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists into strings\n",
    "\n",
    "firstPeriodString = ' '.join(firstPeriodChunks)\n",
    "secondPeriodString = ' '.join(secondPeriodChunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f2ef47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "firstPeriodTokens = tokenizer.tokenize(firstPeriodString)\n",
    "secondPeriodTokens = tokenizer.tokenize(secondPeriodString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54b2ed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in first period:  833072\n",
      "Total tokens in second period:  243876\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens in first period: \" , len(firstPeriodTokens))\n",
    "print(\"Total tokens in second period: \" , len(secondPeriodTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ccdfb29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally up unique tokens\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "firstPeriodTallies = Counter()\n",
    "for word in firstPeriodTokens:\n",
    "    firstPeriodTallies[word] += 1\n",
    "\n",
    "secondPeriodTallies = Counter()\n",
    "for word in secondPeriodTokens:\n",
    "    secondPeriodTallies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b173d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort tallies and reduce to top n tokens\n",
    "\n",
    "firstPeriodTallies = firstPeriodTallies.most_common()[0:500]\n",
    "secondPeriodTallies = secondPeriodTallies.most_common()[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9e2bfc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstPeriodTallies[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bc14429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for top first period keyword  tallies\n",
    "\n",
    "firstTalliesDF = pd.DataFrame(firstPeriodTallies, columns =['Token', 'FirstPeriodRawFreq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc7e20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for top first period keyword  tallies\n",
    "\n",
    "secondTalliesDF = pd.DataFrame(secondPeriodTallies, columns =['Token', 'SecondPeriodRawFreq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "376b5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentChangeDF = pd.merge(firstTalliesDF, secondTalliesDF, on=\"Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10c0b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide raw frequencies by total number of tokens for each period\n",
    "\n",
    "percentChangeDF[\"FirstPeriodRelativeFreq\"] = percentChangeDF[\"FirstPeriodRawFreq\"] / len(firstPeriodTokens)\n",
    "percentChangeDF[\"SecondPeriodRelativeFreq\"] = percentChangeDF[\"SecondPeriodRawFreq\"] / len(secondPeriodTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25f11234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculuate percentage change between first and second period\n",
    "percentChangeDF[\"PercentageChange\"] = (percentChangeDF[\"SecondPeriodRelativeFreq\"] - percentChangeDF[\"FirstPeriodRelativeFreq\"]) / percentChangeDF[\"FirstPeriodRelativeFreq\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd1b7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 1-character tokens\n",
    "\n",
    "percentChangeDF = percentChangeDF[percentChangeDF['Token'].map(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3d17bf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>FirstPeriodRawFreq</th>\n",
       "      <th>SecondPeriodRawFreq</th>\n",
       "      <th>FirstPeriodRelativeFreq</th>\n",
       "      <th>SecondPeriodRelativeFreq</th>\n",
       "      <th>PercentageChange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>fog</td>\n",
       "      <td>322</td>\n",
       "      <td>194</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>105.806617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>ch</td>\n",
       "      <td>741</td>\n",
       "      <td>442</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>103.759345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>continu</td>\n",
       "      <td>230</td>\n",
       "      <td>130</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>93.076310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>object</td>\n",
       "      <td>254</td>\n",
       "      <td>134</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>80.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>figur</td>\n",
       "      <td>315</td>\n",
       "      <td>152</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>64.833890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>narrat</td>\n",
       "      <td>809</td>\n",
       "      <td>386</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>62.986734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>paper</td>\n",
       "      <td>233</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>59.802677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>victorian</td>\n",
       "      <td>289</td>\n",
       "      <td>135</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>59.569322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>bucket</td>\n",
       "      <td>392</td>\n",
       "      <td>183</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>59.469818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>question</td>\n",
       "      <td>277</td>\n",
       "      <td>123</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>51.683666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>turn</td>\n",
       "      <td>369</td>\n",
       "      <td>162</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>49.969217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>particular</td>\n",
       "      <td>283</td>\n",
       "      <td>124</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>49.674813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>tale</td>\n",
       "      <td>208</td>\n",
       "      <td>91</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>49.448490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>keep</td>\n",
       "      <td>219</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>46.621350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>father</td>\n",
       "      <td>324</td>\n",
       "      <td>139</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>46.549137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>offer</td>\n",
       "      <td>237</td>\n",
       "      <td>101</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>45.574901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>polit</td>\n",
       "      <td>244</td>\n",
       "      <td>103</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>44.198543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>scene</td>\n",
       "      <td>380</td>\n",
       "      <td>160</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>43.830126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>includ</td>\n",
       "      <td>244</td>\n",
       "      <td>102</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>42.798557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>differ</td>\n",
       "      <td>352</td>\n",
       "      <td>147</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>42.655377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token  FirstPeriodRawFreq  SecondPeriodRawFreq  \\\n",
       "298         fog                 322                  194   \n",
       "118          ch                 741                  442   \n",
       "397     continu                 230                  130   \n",
       "370      object                 254                  134   \n",
       "309       figur                 315                  152   \n",
       "106      narrat                 809                  386   \n",
       "391       paper                 233                  109   \n",
       "334   victorian                 289                  135   \n",
       "242      bucket                 392                  183   \n",
       "350    question                 277                  123   \n",
       "260        turn                 369                  162   \n",
       "341  particular                 283                  124   \n",
       "419        tale                 208                   91   \n",
       "408        keep                 219                   94   \n",
       "295      father                 324                  139   \n",
       "384       offer                 237                  101   \n",
       "382       polit                 244                  103   \n",
       "250       scene                 380                  160   \n",
       "381      includ                 244                  102   \n",
       "269      differ                 352                  147   \n",
       "\n",
       "     FirstPeriodRelativeFreq  SecondPeriodRelativeFreq  PercentageChange  \n",
       "298                 0.000387                  0.000795        105.806617  \n",
       "118                 0.000889                  0.001812        103.759345  \n",
       "397                 0.000276                  0.000533         93.076310  \n",
       "370                 0.000305                  0.000549         80.212353  \n",
       "309                 0.000378                  0.000623         64.833890  \n",
       "106                 0.000971                  0.001583         62.986734  \n",
       "391                 0.000280                  0.000447         59.802677  \n",
       "334                 0.000347                  0.000554         59.569322  \n",
       "242                 0.000471                  0.000750         59.469818  \n",
       "350                 0.000333                  0.000504         51.683666  \n",
       "260                 0.000443                  0.000664         49.969217  \n",
       "341                 0.000340                  0.000508         49.674813  \n",
       "419                 0.000250                  0.000373         49.448490  \n",
       "408                 0.000263                  0.000385         46.621350  \n",
       "295                 0.000389                  0.000570         46.549137  \n",
       "384                 0.000284                  0.000414         45.574901  \n",
       "382                 0.000293                  0.000422         44.198543  \n",
       "250                 0.000456                  0.000656         43.830126  \n",
       "381                 0.000293                  0.000418         42.798557  \n",
       "269                 0.000423                  0.000603         42.655377  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output 20 biggest increases\n",
    "\n",
    "percentChangeDF.sort_values(by=[\"PercentageChange\"], ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "966b5d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>FirstPeriodRawFreq</th>\n",
       "      <th>SecondPeriodRawFreq</th>\n",
       "      <th>FirstPeriodRelativeFreq</th>\n",
       "      <th>SecondPeriodRelativeFreq</th>\n",
       "      <th>PercentageChange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>mrs</td>\n",
       "      <td>825</td>\n",
       "      <td>134</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>-44.516439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>tulkinghorn</td>\n",
       "      <td>420</td>\n",
       "      <td>71</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>-42.253917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>american</td>\n",
       "      <td>374</td>\n",
       "      <td>68</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-37.891537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>letter</td>\n",
       "      <td>509</td>\n",
       "      <td>95</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>-36.244259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>general</td>\n",
       "      <td>413</td>\n",
       "      <td>80</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-33.831177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>whole</td>\n",
       "      <td>341</td>\n",
       "      <td>67</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>-32.882789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>richard</td>\n",
       "      <td>422</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-32.004478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>studi</td>\n",
       "      <td>540</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>-31.680690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>societi</td>\n",
       "      <td>314</td>\n",
       "      <td>63</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>-31.463113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>court</td>\n",
       "      <td>525</td>\n",
       "      <td>106</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>-31.030030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>head</td>\n",
       "      <td>307</td>\n",
       "      <td>63</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>-29.900382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>krook</td>\n",
       "      <td>403</td>\n",
       "      <td>83</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>-29.646368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>made</td>\n",
       "      <td>668</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-29.430653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>author</td>\n",
       "      <td>381</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-29.170269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>jarndyc</td>\n",
       "      <td>676</td>\n",
       "      <td>141</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>-28.749832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>sir</td>\n",
       "      <td>420</td>\n",
       "      <td>88</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-28.427390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>play</td>\n",
       "      <td>433</td>\n",
       "      <td>91</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>-28.209501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>moral</td>\n",
       "      <td>320</td>\n",
       "      <td>68</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-27.410733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>certain</td>\n",
       "      <td>303</td>\n",
       "      <td>65</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-26.720212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>dark</td>\n",
       "      <td>275</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>-24.227675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Token  FirstPeriodRawFreq  SecondPeriodRawFreq  \\\n",
       "101          mrs                 825                  134   \n",
       "225  tulkinghorn                 420                   71   \n",
       "256     american                 374                   68   \n",
       "175       letter                 509                   95   \n",
       "229      general                 413                   80   \n",
       "276        whole                 341                   67   \n",
       "222      richard                 422                   84   \n",
       "163        studi                 540                  108   \n",
       "310      societi                 314                   63   \n",
       "167        court                 525                  106   \n",
       "317         head                 307                   63   \n",
       "238        krook                 403                   83   \n",
       "136         made                 668                  138   \n",
       "248       author                 381                   79   \n",
       "134      jarndyc                 676                  141   \n",
       "226          sir                 420                   88   \n",
       "214         play                 433                   91   \n",
       "303        moral                 320                   68   \n",
       "322      certain                 303                   65   \n",
       "352         dark                 275                   61   \n",
       "\n",
       "     FirstPeriodRelativeFreq  SecondPeriodRelativeFreq  PercentageChange  \n",
       "101                 0.000990                  0.000549        -44.516439  \n",
       "225                 0.000504                  0.000291        -42.253917  \n",
       "256                 0.000449                  0.000279        -37.891537  \n",
       "175                 0.000611                  0.000390        -36.244259  \n",
       "229                 0.000496                  0.000328        -33.831177  \n",
       "276                 0.000409                  0.000275        -32.882789  \n",
       "222                 0.000507                  0.000344        -32.004478  \n",
       "163                 0.000648                  0.000443        -31.680690  \n",
       "310                 0.000377                  0.000258        -31.463113  \n",
       "167                 0.000630                  0.000435        -31.030030  \n",
       "317                 0.000369                  0.000258        -29.900382  \n",
       "238                 0.000484                  0.000340        -29.646368  \n",
       "136                 0.000802                  0.000566        -29.430653  \n",
       "248                 0.000457                  0.000324        -29.170269  \n",
       "134                 0.000811                  0.000578        -28.749832  \n",
       "226                 0.000504                  0.000361        -28.427390  \n",
       "214                 0.000520                  0.000373        -28.209501  \n",
       "303                 0.000384                  0.000279        -27.410733  \n",
       "322                 0.000364                  0.000267        -26.720212  \n",
       "352                 0.000330                  0.000250        -24.227675  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output 20 biggest decreases\n",
    "\n",
    "percentChangeDF.sort_values(by=[\"PercentageChange\"])[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e87d1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output just tokens and percentage changes for top 20 increases and decreases\n",
    "\n",
    "percentChangeDF[[\"Token\", \"PercentageChange\"]].sort_values(by=[\"PercentageChange\"], ascending=False)[0:20].to_csv(f'{resultsDir}/{projectName}_biggestIncreases_{transitionPoint}.csv',index=False)\n",
    "\n",
    "percentChangeDF[[\"Token\", \"PercentageChange\"]].sort_values(by=[\"PercentageChange\"])[0:20].to_csv(f'{resultsDir}/{projectName}_biggestDecreases_{transitionPoint}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1a456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9105125",
   "metadata": {},
   "source": [
    "# TBD: run tf-idf on filtered version of df compared to rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: re-import JSONL file as pandas dataframe\n",
    "df = pd.read_csv('my_data_with_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04d2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fullContextChunks\"] = df[\"contextChunksLeft\"] + \" \" + df[\"contextChunksRight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91ac3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6ea7172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rther comparison which illumin what meredith i...\n",
       "1    h a littl squeak of the hinges\" (p. 5), and th...\n",
       "2     dalloway from an acquaint pass by, which prov...\n",
       "3    he air. (p. 6) the leaden circl are to provid ...\n",
       "4    e and make it possibl for the reader to map th...\n",
       "Name: stemmed, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['stemmed'] = df.fullContextChunks.map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "df.stemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b88c7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10a61bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 2), stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 2), stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 2), stop_words='english')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(stop_words='english', min_df=2, max_df=0.5, ngram_range=(1,2))\n",
    "cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "abd61388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comparison', 6963),\n",
       " ('illumin', 15414),\n",
       " ('meredith', 19899),\n",
       " ('provid', 24771),\n",
       " ('begin', 4077),\n",
       " ('mrs', 20710),\n",
       " ('dalloway', 8154),\n",
       " ('virginia', 33543),\n",
       " ('woolf', 34783),\n",
       " ('open', 22031),\n",
       " ('novel', 21433),\n",
       " ('charact', 5663),\n",
       " ('foot', 12951),\n",
       " ('london', 18531),\n",
       " ('appar', 2855),\n",
       " ('arbitrari', 2989),\n",
       " ('sentenc', 27854),\n",
       " ('gradual', 13879),\n",
       " ('explained', 11726),\n",
       " ('fresh', 13216)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate all the n-grams found in all documents\n",
    "from itertools import islice\n",
    "cvec.fit(df.stemmed)\n",
    "list(islice(cvec.vocabulary_.items(), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a07ab4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35472"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many total n-grams we have\n",
    "len(cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bd11921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (1758, 35472)\n",
      "nonzero count: 202159\n",
      "sparsity: 0.32%\n"
     ]
    }
   ],
   "source": [
    "cvec_counts = cvec.transform(df.stemmed)\n",
    "print('sparse matrix shape:', cvec_counts.shape)\n",
    "print('nonzero count:', cvec_counts.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * cvec_counts.nnz / (cvec_counts.shape[0] * cvec_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac869dea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m occ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(cvec_counts\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 2\u001b[0m counts_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccurrences\u001b[39m\u001b[38;5;124m'\u001b[39m: occ})\n\u001b[1;32m      3\u001b[0m counts_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccurrences\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7c613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57009c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a504e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad3eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd96d22f",
   "metadata": {},
   "source": [
    "# OPTIONAL: filter results down to specific passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that can compare two intervals in the form of listed tuples, eg [x0, x1], [y0,y1]\n",
    "def overlaps(range1, range2):\n",
    "    try:\n",
    "        if len(range1) == 2 and len(range2) == 2:\n",
    "            return max(range1[0],range2[0]) < min(range1[1],range2[1])\n",
    "        else:\n",
    "            return False\n",
    "    except TypeError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1af4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell specifies the start and end indices of the passage \n",
    "\n",
    "passageName = \"romanticSide\"\n",
    "quoteIndex = [4375, 4407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"doesOverlap\"] = df.apply(lambda x: overlaps(quoteIndex, x['Locations in A']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"doesOverlap\"]]\n",
    "df = df.reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
